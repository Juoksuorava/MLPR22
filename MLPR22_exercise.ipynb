{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TKO_3120 Machine Learning and Pattern Recognition\n",
    "\n",
    "Image recognition exercise\n",
    "\n",
    "Your name <br>\n",
    "Your e-mail\n",
    "\n",
    "February 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the template for the image recognition exercise. <Br>\n",
    "Some **general instructions**:\n",
    " - write a clear *report*, understandable for an unspecialized reader: define shortly the concepts and explain the phases you use\n",
    "    - use the Markdown feature of the notebook for larger explanations\n",
    " - return your output as a working Jupyter notebook\n",
    " - name your file as MLPR22_exercise_your_surname.ipynb\n",
    " - write easily readable code with comments     \n",
    "     - if you exploit some code from web, provide a reference\n",
    "     - avoid redundant code! Exploit the relevant parts and modify the code for your purposes to produce only what you need \n",
    " - it is ok to discuss with a friend about the assignment. But it is not ok to copy someone's work. Everyone should submit their own implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deadline 21st of March at 23:59**\n",
    "- No extension granted, unless you have an extremely justified reason. In such case, ask for extension well in advance!\n",
    "- Start now, do not leave it to the last minute. This exercise will need some labour!\n",
    "- If you encounter problems, Google first and if you canâ€™t find an answer, ask for help\n",
    "    - pekavir@utu.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading**\n",
    "\n",
    "The exercise covers a part of the grading in this course. The course exam has 5 questions, 6 points of each. Exercise gives 4 points, i.e. the total score is 34 points.\n",
    "\n",
    "From the template below, you can see how many exercise points can be acquired from each task. Exam points are given according to the table below: <br>\n",
    "<br>\n",
    "7-8 exercise points: 1 point <br>\n",
    "9-10 exercise points: 2 points <br>\n",
    "11-12 exercise points: 3 points <br>\n",
    "13-14 exercise points: 4 points <br>\n",
    "<br>\n",
    "To pass the exercise, you need at least 7 exercise points, distributed somewhat evenly into tasks (you can't just implement Introduction, Data preparation and Feature extraction and leave the left undone!) <Br>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an introductory chapter for your report **(1 p)**\n",
    "<br>E.g.\n",
    "- What is the purpose of this task?\n",
    "- What kind of data were used? Where did it originate?\n",
    "- Which methods did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images: https://unsplash.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all packages needed here\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_trees = np.loadtxt('data/trees.txt', dtype='U150')\n",
    "urls_pebbles = np.loadtxt('data/pebbles.txt', dtype='U150')\n",
    "urls_sky = np.loadtxt('data/sky.txt', dtype='U150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_flip_images(data, id):\n",
    "    i = 0\n",
    "    img_data = []\n",
    "    img_id = []\n",
    "    for img in data:\n",
    "        temp_img = io.imread(img)\n",
    "        img_data.append(temp_img)\n",
    "        img_id.append(id + str(i))\n",
    "        np.flip(temp_img, 1)\n",
    "        img_data.append(temp_img)\n",
    "        img_id.append(id + str(i))\n",
    "        i += 1\n",
    "    print(id + ' done')\n",
    "    return img_data, img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees done\n",
      "pebbles done\n",
      "sky done\n"
     ]
    }
   ],
   "source": [
    "trees_data, trees_id = load_and_flip_images(urls_trees, 'trees')\n",
    "pebbles_data, pebbles_id = load_and_flip_images(urls_pebbles, 'pebbles')\n",
    "sky_data, sky_id = load_and_flip_images(urls_sky, 'sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dimension(data):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    for img in data:\n",
    "        rows.append(len(img))\n",
    "        columns.append(len(img[0]))\n",
    "    return rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_x, trees_y = extract_dimension(trees_data)\n",
    "pebbles_x, pebbles_y = extract_dimension(pebbles_data)\n",
    "sky_x, sky_y = extract_dimension(sky_data)\n",
    "\n",
    "x_dimensions = trees_x + pebbles_x + sky_x\n",
    "y_dimensions = trees_y + pebbles_y + sky_y\n",
    "\n",
    "mean_x = np.mean(x_dimensions)\n",
    "mean_y = np.mean(y_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(data):\n",
    "    resized_images = []\n",
    "    for img in data:\n",
    "        resized_images.append(resize(img, (mean_x, mean_y)))\n",
    "    print('done')\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "resized_trees = resize_images(trees_data)\n",
    "resized_pebbles = resize_images(pebbles_data)\n",
    "resized_sky = resize_images(sky_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_images(data):\n",
    "    grayscaled = []\n",
    "    for img in data:\n",
    "        grayscaled.append(rgb2gray(img))\n",
    "    print ('done')\n",
    "    return grayscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "grayscaled_resized_trees = grayscale_images(resized_trees)\n",
    "grayscaled_resized_pebbles = grayscale_images(resized_pebbles)\n",
    "grayscaled_resized_sky = grayscale_images(resized_sky)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform preparations for the data **(3 p)**\n",
    "- import all the packages needed for this notebook in one cell\n",
    "- read the URL:s from the text files and import the images\n",
    "- crop and/or resize the images into same size\n",
    "- for GLCM and GLRLM, change the images into grayscale and reduce the quantization level to 8 levels\n",
    "- make data augmentation: flip each image horizontally to increase the number of examples in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First order texture measures (6 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the below mentioned color features for each image **(1 p)**\n",
    "    - Mean for each RGB color channel\n",
    "    - Variance for each RGB color channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second order texture measures (10 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate feature values for each following feature for each image in the prepared data set:\n",
    "- Gray-Level-Co-Occurrence (GLCM) features (4 features) **(2 p)**\n",
    "    - calculate the \"correlation\" feature using the GLC matrix\n",
    "        - in horizontal and vertical directions for two reference pixel distances (you can choose the distances)\n",
    "    - explain your choice for the distances\n",
    "- Gray-Level-Run-Length (GLRL) features (6 features) **(2 p)**\n",
    "    - Calculate the following three features in horizontal and vertical direction\n",
    "        - Use the given function for Gray-Level-Run-Length (GLRL) matrix\n",
    "        - Implement the following run-length features using the GLRL matrix\n",
    "            - Short-Run emphasis\n",
    "            - Long-run emphasis\n",
    "            - Run percentage\n",
    "        - Test your implementation with the given toy image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather your features into an input array X, and the image classes into an output array y. Assign an image id for each image so that the original and flipped image have the same id. Standardize the feature values in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grey-Level-Run-Length-Matrix\n",
    "\n",
    "def glrlm(image, levels, angle):\n",
    "    if angle==0: # horizontal        \n",
    "        runs=image.shape[1]\n",
    "        glrl_matrix=np.zeros([levels,runs])\n",
    "\n",
    "        for row in range(0,image.shape[0]):\n",
    "            onerow=image[row,:]\n",
    "            counts=[(i, len(list(g))) for i, g in groupby(onerow)]\n",
    "            for count in counts:\n",
    "                glrl_matrix[count[0],count[1]-1]=glrl_matrix[count[0],count[1]-1]+1\n",
    "\n",
    "    if angle==90: # vertical\n",
    "        runs=image.shape[0]\n",
    "        glrl_matrix=np.zeros([levels,runs])\n",
    "\n",
    "        for column in range(0,image.shape[1]):\n",
    "            onecolumn=image[:,column]\n",
    "            counts=[(i, len(list(g))) for i, g in groupby(onecolumn)]\n",
    "            for count in counts:\n",
    "                glrl_matrix[count[0],count[1]-1]=glrl_matrix[count[0],count[1]-1]+1\n",
    "        \n",
    "    return(glrl_matrix)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_m = gray-level-run-length-matrix\n",
    "# Np = the number of pixels in the image\n",
    "def emphasis(G_m, Np):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return(SRE, LRE, RP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLRL matrix for 0 degrees:\n",
      "[[1. 1. 0. 0.]\n",
      " [2. 0. 1. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "GLRL matrix for 90 degrees:\n",
      "[[1. 1. 0.]\n",
      " [5. 0. 0.]\n",
      " [4. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test the glrlm function with a toy example 1\n",
    "toy_image=np.array([[1,1,1,2],[2,0,0,1],[1,0,2,2]])\n",
    "\n",
    "toy_GLRLM_0=glrlm(toy_image,3, 0)\n",
    "toy_GLRLM_90=glrlm(toy_image,3, 90)\n",
    "\n",
    "print('GLRL matrix for 0 degrees:')\n",
    "print(toy_GLRLM_0)\n",
    "print('GLRL matrix for 90 degrees:')\n",
    "print(toy_GLRLM_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mh:\\MLPR22\\MLPR22_exercise.ipynb Cell 29'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39m# test your emphasis function in 0 direction with toy example 1\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000027?line=1'>2</a>\u001b[0m toy_SRE_0, toy_LRE_0, toy_RP_0\u001b[39m=\u001b[39memphasis(toy_GLRLM_0, \u001b[39m12\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000027?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSRE:\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mround(toy_SRE_0, \u001b[39m3\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000027?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLRE:\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mround(toy_LRE_0, \u001b[39m3\u001b[39m))\n",
      "\u001b[1;32mh:\\MLPR22\\MLPR22_exercise.ipynb Cell 27'\u001b[0m in \u001b[0;36memphasis\u001b[1;34m(G_m, Np)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000025?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memphasis\u001b[39m(G_m, Np):\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000025?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/MLPR22/MLPR22_exercise.ipynb#ch0000025?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m(SRE, LRE, RP)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SRE' is not defined"
     ]
    }
   ],
   "source": [
    "# test your emphasis function in 0 direction with toy example 1\n",
    "toy_SRE_0, toy_LRE_0, toy_RP_0=emphasis(toy_GLRLM_0, 12)\n",
    "print('SRE:', np.round(toy_SRE_0, 3))\n",
    "print('LRE:', np.round(toy_LRE_0, 3))\n",
    "print('RP:', np.round(toy_RP_0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.932\n",
      "LRE: 1.273\n",
      "RP: 0.917\n"
     ]
    }
   ],
   "source": [
    "# test the emphasis function in 90 direction with toy example 1\n",
    "toy_SRE_90, toy_LRE_90, toy_RP_90=emphasis(toy_GLRLM_90, 12)\n",
    "print('SRE:', np.round(toy_SRE_90, 3))\n",
    "print('LRE:', np.round(toy_LRE_90, 3))\n",
    "print('RP:', np.round(toy_RP_90, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLRL matrix for 0 degrees:\n",
      "[[1. 1. 0. 1.]\n",
      " [2. 0. 1. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "GLRL matrix for 90 degrees:\n",
      "[[4. 0. 1. 0.]\n",
      " [5. 0. 0. 0.]\n",
      " [4. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test the glrlm function with a toy example 2\n",
    "toy_image=np.array([[1,1,1,2],[2,0,0,1],[1,0,2,2],[0,0,0,0]])\n",
    "\n",
    "toy_GLRLM_0=glrlm(toy_image,3, 0)\n",
    "toy_GLRLM_90=glrlm(toy_image,3, 90)\n",
    "\n",
    "print('GLRL matrix for 0 degrees:')\n",
    "print(toy_GLRLM_0)\n",
    "print('GLRL matrix for 90 degrees:')\n",
    "print(toy_GLRLM_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.63\n",
      "LRE: 4.222\n",
      "RP: 0.562\n"
     ]
    }
   ],
   "source": [
    "# test your emphasis function in 0 direction with toy example 2\n",
    "toy_SRE_0, toy_LRE_0, toy_RP_0=emphasis(toy_GLRLM_0, 16)\n",
    "print('SRE:', np.round(toy_SRE_0, 3))\n",
    "print('LRE:', np.round(toy_LRE_0, 3))\n",
    "print('RP:', np.round(toy_RP_0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.937\n",
      "LRE: 1.571\n",
      "RP: 0.875\n"
     ]
    }
   ],
   "source": [
    "# test the emphasis function in 90 direction with toy example 2\n",
    "toy_SRE_90, toy_LRE_90, toy_RP_90=emphasis(toy_GLRLM_90, 16)\n",
    "print('SRE:', np.round(toy_SRE_90, 3))\n",
    "print('LRE:', np.round(toy_LRE_90, 3))\n",
    "print('RP:', np.round(toy_RP_90, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make illustrations of the feature relationships, and discuss the results **(1 p)**\n",
    "- Pairplot \n",
    "    - Which feature pairs possess roughly linear dependence?\n",
    "- PCA\n",
    "    - Can you see any clusters in PCA?\n",
    "    - Does this figure give you any clues, how well you will be able to classify the image types? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the classifiers and estimate their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifiers and estimate their perfomance. Use LeaveOneGroupOut or GroupKFold cross validator (image id as group indicator).\n",
    "- k Nearest Neighbors classifier **(1 p)** \n",
    "    - optimize the hyperparameter (k) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix\n",
    "- Regularized linear model with Ridge regression **(1 p)**\n",
    "    - optimize the hyperparameter (alpha) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix\n",
    "\n",
    "- Multi-layer perceptron MLP **(1 p)**\n",
    "    - build the classifier. Use:\n",
    "        - 1 hidden layer\n",
    "        - solver for weight optimization: stochastic gradient-based optimizer ('adam')\n",
    "        - activation function for the hidden layer: rectified linear unit function ('relu')\n",
    "        - Early stop\n",
    "    - optimize the number of neurons in the hidden layer and select the best model for the classifier\n",
    "    - use Early stop committee, i.e. after selecting the model, calculate the prediction for the test data several times with different sampling of the training data. The members of the committee vote for the predicted class of the test sample. Use 50% of the training data for validation (algorithm terminates the training when validation score is not improving)\n",
    "    - estimate the performance of the classifier with nested cross validation\n",
    "- Discuss your results **(1 p)**\n",
    "<br>E.g.\n",
    "    - Which model performs the best and why?\n",
    "    - What are the limitations?\n",
    "    - How could the results be improved?\n",
    "    - How do you expect the models will perform with unseen data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

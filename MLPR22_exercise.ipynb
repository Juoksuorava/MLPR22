{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TKO_3120 Machine Learning and Pattern Recognition\n",
    "\n",
    "Image recognition exercise\n",
    "\n",
    "Your name <br>\n",
    "Your e-mail\n",
    "\n",
    "February 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the template for the image recognition exercise. <Br>\n",
    "Some **general instructions**:\n",
    " - write a clear *report*, understandable for an unspecialized reader: define shortly the concepts and explain the phases you use\n",
    "    - use the Markdown feature of the notebook for larger explanations\n",
    " - return your output as a working Jupyter notebook\n",
    " - name your file as MLPR22_exercise_your_surname.ipynb\n",
    " - write easily readable code with comments     \n",
    "     - if you exploit some code from web, provide a reference\n",
    "     - avoid redundant code! Exploit the relevant parts and modify the code for your purposes to produce only what you need \n",
    " - it is ok to discuss with a friend about the assignment. But it is not ok to copy someone's work. Everyone should submit their own implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deadline 21st of March at 23:59**\n",
    "- No extension granted, unless you have an extremely justified reason. In such case, ask for extension well in advance!\n",
    "- Start now, do not leave it to the last minute. This exercise will need some labour!\n",
    "- If you encounter problems, Google first and if you can’t find an answer, ask for help\n",
    "    - pekavir@utu.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading**\n",
    "\n",
    "The exercise covers a part of the grading in this course. The course exam has 5 questions, 6 points of each. Exercise gives 4 points, i.e. the total score is 34 points.\n",
    "\n",
    "From the template below, you can see how many exercise points can be acquired from each task. Exam points are given according to the table below: <br>\n",
    "<br>\n",
    "7-8 exercise points: 1 point <br>\n",
    "9-10 exercise points: 2 points <br>\n",
    "11-12 exercise points: 3 points <br>\n",
    "13-14 exercise points: 4 points <br>\n",
    "<br>\n",
    "To pass the exercise, you need at least 7 exercise points, distributed somewhat evenly into tasks (you can't just implement Introduction, Data preparation and Feature extraction and leave the left undone!) <Br>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an introductory chapter for your report **(1 p)**\n",
    "<br>E.g.\n",
    "- What is the purpose of this task?\n",
    "- What kind of data were used? Where did it originate?\n",
    "- Which methods did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images: https://unsplash.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform preparations for the data **(3 p)**\n",
    "- import all the packages needed for this notebook in one cell\n",
    "- read the URL:s from the text files and import the images\n",
    "- crop and/or resize the images into same size\n",
    "- for GLCM and GLRLM, change the images into grayscale and reduce the quantization level to 8 levels\n",
    "- make data augmentation: flip each image horizontally to increase the number of examples in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all packages needed here\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import graycomatrix\n",
    "from skimage.feature import graycoprops\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_trees = np.loadtxt('data/trees.txt', dtype='U150')\n",
    "urls_pebbles = np.loadtxt('data/pebbles.txt', dtype='U150')\n",
    "urls_sky = np.loadtxt('data/sky.txt', dtype='U150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(data):\n",
    "    img_data = []\n",
    "    for img in tqdm(data):\n",
    "        temp_img = io.imread(img)\n",
    "        img_data.append(temp_img)\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:15<00:00,  3.97it/s]\n",
      "100%|██████████| 60/60 [00:12<00:00,  4.63it/s]\n",
      "100%|██████████| 60/60 [00:10<00:00,  5.54it/s]\n"
     ]
    }
   ],
   "source": [
    "trees_data = load_images(urls_trees)\n",
    "pebbles_data = load_images(urls_pebbles)\n",
    "sky_data = load_images(urls_sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dimension(data):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    for img in tqdm(data):\n",
    "        rows.append(len(img))\n",
    "        columns.append(len(img[0]))\n",
    "    return rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<?, ?it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 120989.54it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 122223.53it/s]\n"
     ]
    }
   ],
   "source": [
    "trees_x, trees_y = extract_dimension(trees_data)\n",
    "pebbles_x, pebbles_y = extract_dimension(pebbles_data)\n",
    "sky_x, sky_y = extract_dimension(sky_data)\n",
    "\n",
    "x_dimensions = trees_x + pebbles_x + sky_x\n",
    "y_dimensions = trees_y + pebbles_y + sky_y\n",
    "\n",
    "mean_x = np.round(np.mean(x_dimensions) / 2)\n",
    "mean_y = np.round(np.mean(y_dimensions) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(data):\n",
    "    resized_images = []\n",
    "    for img in tqdm(data):\n",
    "        resized_images.append(resize(img, (mean_x, mean_y)))\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.89it/s]\n",
      "100%|██████████| 60/60 [00:11<00:00,  5.15it/s]\n",
      "100%|██████████| 60/60 [00:10<00:00,  5.64it/s]\n"
     ]
    }
   ],
   "source": [
    "resized_trees = resize_images(trees_data)\n",
    "resized_pebbles = resize_images(pebbles_data)\n",
    "resized_sky = resize_images(sky_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_images(data):\n",
    "    grayscaled = []\n",
    "    for img in tqdm(data):\n",
    "        grayscaled.append(rgb2gray(img))\n",
    "    return grayscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 407.30it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 392.75it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 432.03it/s]\n"
     ]
    }
   ],
   "source": [
    "grayscaled_resized_trees = grayscale_images(resized_trees)\n",
    "grayscaled_resized_pebbles = grayscale_images(resized_pebbles)\n",
    "grayscaled_resized_sky = grayscale_images(resized_sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_images(data, quants):\n",
    "    quantized_images = []\n",
    "    for img in tqdm(data):\n",
    "        quantized_images.append(np.round(img*(quants-1)).astype(int))\n",
    "    return quantized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:01<00:00, 43.13it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 43.03it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 46.00it/s]\n"
     ]
    }
   ],
   "source": [
    "quantized_trees = quantize_images(grayscaled_resized_trees, 8)\n",
    "quantized_pebbles = quantize_images(grayscaled_resized_pebbles, 8)\n",
    "quantized_sky = quantize_images(grayscaled_resized_sky, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_and_label_images(data, id):\n",
    "    img_id = []\n",
    "    images = []\n",
    "    i = 0\n",
    "    img_labels = []\n",
    "    for img in tqdm(data):\n",
    "        img_labels.append(id)\n",
    "        img_id.append(id + str(i))\n",
    "        images.append(img)\n",
    "        img_labels.append(id)\n",
    "        img_id.append(id + str(i))\n",
    "        images.append(np.flip(img, 1))\n",
    "        i += 1\n",
    "    return images, img_id, img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 120873.31it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 120931.40it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 121047.73it/s]\n"
     ]
    }
   ],
   "source": [
    "final_trees, trees_id, trees_labels = flip_and_label_images(quantized_trees, 'trees')\n",
    "final_pebbles, pebbles_id, pebbles_labels = flip_and_label_images(quantized_pebbles, 'pebbles')\n",
    "final_sky, sky_id, sky_labels = flip_and_label_images(quantized_sky, 'sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 120873.31it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 120815.29it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 120931.40it/s]\n"
     ]
    }
   ],
   "source": [
    "rgb_trees, rgb_trees_id, rgb_trees_labels = flip_and_label_images(resized_trees, 'trees')\n",
    "rgb_pebbles, rgb_pebbles_id, rgb_pebbles_labels = flip_and_label_images(resized_pebbles, 'pebbles')\n",
    "rgb_sky, rgb_sky_id, rgb_sky_labels = flip_and_label_images(resized_sky, 'sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gray = final_trees + final_pebbles + final_sky\n",
    "data_rgb = rgb_trees + rgb_pebbles + rgb_sky\n",
    "data_id = trees_id + pebbles_id + sky_id\n",
    "data_labels = trees_labels + pebbles_labels + sky_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First order texture measures (6 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the below mentioned color features for each image **(1 p)**\n",
    "    - Mean for each RGB color channel\n",
    "    - Variance for each RGB color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rgb_means_and_vars(data):\n",
    "    r_means = []\n",
    "    g_means = []\n",
    "    b_means = []\n",
    "    r_vars = []\n",
    "    g_vars = []\n",
    "    b_vars = []\n",
    "    for img in tqdm(data):\n",
    "        r_vars.append(np.var(img[:,:,0]))\n",
    "        r_means.append(np.mean(img[:,:,0]))\n",
    "        g_vars.append(np.var(img[:,:,1]))\n",
    "        g_means.append(np.mean(img[:,:,1]))\n",
    "        b_vars.append(np.var(img[:,:,2]))\n",
    "        b_means.append(np.mean(img[:,:,2]))\n",
    "    return r_means, g_means, b_means, r_vars, g_vars, b_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:06<00:00, 59.56it/s]\n"
     ]
    }
   ],
   "source": [
    "r_means, g_means, b_means, r_vars, g_vars, b_vars = calc_rgb_means_and_vars(data_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_means_vars = np.transpose(np.array([r_means, g_means, b_means, r_vars, g_vars, b_vars]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second order texture measures (10 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gray-Level-Co-Occurrence (GLCM) features (4 features) **(2 p)**\n",
    "    - calculate the \"correlation\" feature using the GLC matrix\n",
    "        - in horizontal and vertical directions for two reference pixel distances (you can choose the distances)\n",
    "    - explain your choice for the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:01<00:00, 197.24it/s]\n",
      "100%|██████████| 360/360 [00:00<00:00, 6366.80it/s]\n",
      "100%|██████████| 360/360 [00:00<00:00, 727335.95it/s]\n"
     ]
    }
   ],
   "source": [
    "glcm = []\n",
    "distances = [5, 25]\n",
    "\n",
    "for img in tqdm(data_gray):\n",
    "    glcm.append(graycomatrix(img, distances, [0, np.pi/2], 8))\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for img in tqdm(glcm):\n",
    "    correlations.append(graycoprops(img, 'correlation'))\n",
    "\n",
    "correlations1 = []\n",
    "correlations2 = []\n",
    "correlations3 = []\n",
    "correlations4 = []\n",
    "\n",
    "for cor in tqdm(correlations):\n",
    "    correlations1.append(cor[0,0])\n",
    "    correlations2.append(cor[0,1])\n",
    "    correlations3.append(cor[1,0])\n",
    "    correlations4.append(cor[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gray-Level-Run-Length (GLRL) features (6 features) **(2 p)**\n",
    "    - Calculate the following three features in horizontal and vertical direction\n",
    "        - Use the given function for Gray-Level-Run-Length (GLRL) matrix\n",
    "        - Implement the following run-length features using the GLRL matrix\n",
    "            - Short-Run emphasis\n",
    "            - Long-run emphasis\n",
    "            - Run percentage\n",
    "        - Test your implementation with the given toy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grey-Level-Run-Length-Matrix\n",
    "\n",
    "def glrlm(image, levels, angle):\n",
    "    if angle==0: # horizontal        \n",
    "        runs=image.shape[1]\n",
    "        glrl_matrix=np.zeros([levels,runs])\n",
    "\n",
    "        for row in range(0,image.shape[0]):\n",
    "            onerow=image[row,:]\n",
    "            counts=[(i, len(list(g))) for i, g in groupby(onerow)]\n",
    "            for count in counts:\n",
    "                glrl_matrix[count[0],count[1]-1]=glrl_matrix[count[0],count[1]-1]+1\n",
    "\n",
    "    if angle==90: # vertical\n",
    "        runs=image.shape[0]\n",
    "        glrl_matrix=np.zeros([levels,runs])\n",
    "\n",
    "        for column in range(0,image.shape[1]):\n",
    "            onecolumn=image[:,column]\n",
    "            counts=[(i, len(list(g))) for i, g in groupby(onecolumn)]\n",
    "            for count in counts:\n",
    "                glrl_matrix[count[0],count[1]-1]=glrl_matrix[count[0],count[1]-1]+1\n",
    "        \n",
    "    return(glrl_matrix)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_m = gray-level-run-length-matrix\n",
    "# Np = the number of pixels in the image\n",
    "def emphasis(G_m, Np):\n",
    "    \n",
    "    # initialize summing\n",
    "    SRE_sum = 0\n",
    "    LRE_sum = 0\n",
    "    denominator = 0\n",
    "    RP_sum = 0\n",
    "\n",
    "    # limits for looping\n",
    "    N_g = len(G_m)\n",
    "    N_r = len(G_m[0])\n",
    "\n",
    "    for i in range(N_g):\n",
    "        for j in range(N_r):\n",
    "            # calculate sums\n",
    "            denominator += G_m[i,j]\n",
    "            SRE_sum += G_m[i,j]/(j+1)**2   # j+1 since the indexing is different \n",
    "            LRE_sum += G_m[i,j]*(j+1)**2    # in the formulas (from 1...Nr, not 0...nr-1)\n",
    "            RP_sum += G_m[i,j]\n",
    "\n",
    "    # do the divisions according to the formulas\n",
    "    SRE = SRE_sum/denominator\n",
    "    LRE = LRE_sum/denominator\n",
    "    RP = RP_sum/Np\n",
    "    \n",
    "    return(SRE, LRE, RP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLRL matrix for 0 degrees:\n",
      "[[1. 1. 0. 0.]\n",
      " [2. 0. 1. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "GLRL matrix for 90 degrees:\n",
      "[[1. 1. 0.]\n",
      " [5. 0. 0.]\n",
      " [4. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test the glrlm function with a toy example 1\n",
    "toy_image=np.array([[1,1,1,2],[2,0,0,1],[1,0,2,2]])\n",
    "\n",
    "toy_GLRLM_0=glrlm(toy_image,3, 0)\n",
    "toy_GLRLM_90=glrlm(toy_image,3, 90)\n",
    "\n",
    "print('GLRL matrix for 0 degrees:')\n",
    "print(toy_GLRLM_0)\n",
    "print('GLRL matrix for 90 degrees:')\n",
    "print(toy_GLRLM_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.701\n",
      "LRE: 2.75\n",
      "RP: 0.667\n"
     ]
    }
   ],
   "source": [
    "# test your emphasis function in 0 direction with toy example 1\n",
    "toy_SRE_0, toy_LRE_0, toy_RP_0=emphasis(toy_GLRLM_0, 12)\n",
    "print('SRE:', np.round(toy_SRE_0, 3))\n",
    "print('LRE:', np.round(toy_LRE_0, 3))\n",
    "print('RP:', np.round(toy_RP_0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.932\n",
      "LRE: 1.273\n",
      "RP: 0.917\n"
     ]
    }
   ],
   "source": [
    "# test the emphasis function in 90 direction with toy example 1\n",
    "toy_SRE_90, toy_LRE_90, toy_RP_90=emphasis(toy_GLRLM_90, 12)\n",
    "print('SRE:', np.round(toy_SRE_90, 3))\n",
    "print('LRE:', np.round(toy_LRE_90, 3))\n",
    "print('RP:', np.round(toy_RP_90, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLRL matrix for 0 degrees:\n",
      "[[1. 1. 0. 1.]\n",
      " [2. 0. 1. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "GLRL matrix for 90 degrees:\n",
      "[[4. 0. 1. 0.]\n",
      " [5. 0. 0. 0.]\n",
      " [4. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test the glrlm function with a toy example 2\n",
    "toy_image=np.array([[1,1,1,2],[2,0,0,1],[1,0,2,2],[0,0,0,0]])\n",
    "\n",
    "toy_GLRLM_0=glrlm(toy_image,3, 0)\n",
    "toy_GLRLM_90=glrlm(toy_image,3, 90)\n",
    "\n",
    "print('GLRL matrix for 0 degrees:')\n",
    "print(toy_GLRLM_0)\n",
    "print('GLRL matrix for 90 degrees:')\n",
    "print(toy_GLRLM_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.63\n",
      "LRE: 4.222\n",
      "RP: 0.562\n"
     ]
    }
   ],
   "source": [
    "# test your emphasis function in 0 direction with toy example 2\n",
    "toy_SRE_0, toy_LRE_0, toy_RP_0=emphasis(toy_GLRLM_0, 16)\n",
    "print('SRE:', np.round(toy_SRE_0, 3))\n",
    "print('LRE:', np.round(toy_LRE_0, 3))\n",
    "print('RP:', np.round(toy_RP_0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRE: 0.937\n",
      "LRE: 1.571\n",
      "RP: 0.875\n"
     ]
    }
   ],
   "source": [
    "# test the emphasis function in 90 direction with toy example 2\n",
    "toy_SRE_90, toy_LRE_90, toy_RP_90=emphasis(toy_GLRLM_90, 16)\n",
    "print('SRE:', np.round(toy_SRE_90, 3))\n",
    "print('LRE:', np.round(toy_LRE_90, 3))\n",
    "print('RP:', np.round(toy_RP_90, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather your features into an input array X, and the image classes into an output array y. Assign an image id for each image so that the original and flipped image have the same id. Standardize the feature values in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make illustrations of the feature relationships, and discuss the results **(1 p)**\n",
    "- Pairplot \n",
    "    - Which feature pairs possess roughly linear dependence?\n",
    "- PCA\n",
    "    - Can you see any clusters in PCA?\n",
    "    - Does this figure give you any clues, how well you will be able to classify the image types? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the classifiers and estimate their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the classifiers and estimate their perfomance. Use LeaveOneGroupOut or GroupKFold cross validator (image id as group indicator).\n",
    "- k Nearest Neighbors classifier **(1 p)** \n",
    "    - optimize the hyperparameter (k) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_and_score(X, y, kSet, groups):\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    best_k = kSet[0]\n",
    "    best_acc = 0\n",
    "    acc = []\n",
    "\n",
    "    for k in kSet:\n",
    "        temp_preds = []\n",
    "        temp_tests = []\n",
    "        for train, test in logo.split(X, y, groups = groups):\n",
    "            X_train, X_test = X[train], X[test]\n",
    "            y_train, y_test = y[train], y[test]\n",
    "            kNN = KNeighborsClassifier(n_neighbors = k)\n",
    "            kNN.fit(X_train, y_train)\n",
    "            y_preds = kNN.predict(X_test)\n",
    "            temp_preds.append(y_preds)\n",
    "            temp_tests.append(y_test)\n",
    "        preds = [i for l in temp_preds for i in l]\n",
    "        tests = [i for l in temp_tests for i in l]\n",
    "        acc.append(metrics.accuracy_score(tests, preds))\n",
    "\n",
    "    for i in range(len(acc)):\n",
    "        if acc[i] > best_acc:\n",
    "            best_k = kSet[i]\n",
    "            best_acc = acc[i]\n",
    "    return best_k, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlogocv_acc_cf(X, y, groups, n):\n",
    "\n",
    "    gkf = GroupKFold(n_splits = n)\n",
    "    acc = []\n",
    "    cf = []\n",
    "    kSet = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "    for train, test in tqdm(gkf.split(X, y, groups = groups)):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = np.array(y)[train], np.array(y)[test]\n",
    "        groups_train = np.array(groups)[train]\n",
    "        best_k, best_acc = find_best_k_and_score(X_train, y_train, kSet, groups_train)\n",
    "        kNN = KNeighborsClassifier(n_neighbors = best_k)\n",
    "        kNN.fit(X_train, y_train)\n",
    "        y_preds = kNN.predict(X_test)\n",
    "        acc.append(metrics.accuracy_score(y_test, y_preds))\n",
    "        cf.append(metrics.confusion_matrix(y_test, y_preds))\n",
    "    \n",
    "    mean_acc = np.mean(acc)\n",
    "    mean_cf = np.sum(cf, axis = 0)\n",
    "\n",
    "    return mean_acc, np.round(mean_cf / n).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularized linear model with Ridge regression **(1 p)**\n",
    "    - optimize the hyperparameter (alpha) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix\n",
    "\n",
    "- Multi-layer perceptron MLP **(1 p)**\n",
    "    - build the classifier. Use:\n",
    "        - 1 hidden layer\n",
    "        - solver for weight optimization: stochastic gradient-based optimizer ('adam')\n",
    "        - activation function for the hidden layer: rectified linear unit function ('relu')\n",
    "        - Early stop\n",
    "    - optimize the number of neurons in the hidden layer and select the best model for the classifier\n",
    "    - use Early stop committee, i.e. after selecting the model, calculate the prediction for the test data several times with different sampling of the training data. The members of the committee vote for the predicted class of the test sample. Use 50% of the training data for validation (algorithm terminates the training when validation score is not improving)\n",
    "    - estimate the performance of the classifier with nested cross validation\n",
    "- Discuss your results **(1 p)**\n",
    "<br>E.g.\n",
    "    - Which model performs the best and why?\n",
    "    - What are the limitations?\n",
    "    - How could the results be improved?\n",
    "    - How do you expect the models will perform with unseen data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
